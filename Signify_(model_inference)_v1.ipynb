{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1XSCLhNeu3dHmnVZIuI31zdr-cZniTXZr","authorship_tag":"ABX9TyO1n1XauePzXPqNf1L5FywB"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!pip install mediapipe -q\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"UnfVBmXoKP5b","executionInfo":{"status":"ok","timestamp":1730694884133,"user_tz":-330,"elapsed":28880,"user":{"displayName":"Kaushi Gihan ML","userId":"11214181140146971518"}},"outputId":"4ada081f-28d6-40d4-a9e4-42fbfcac8e06"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m36.1/36.1 MB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","tensorflow-metadata 1.16.1 requires protobuf<4.21,>=3.20.3; python_version < \"3.11\", but you have protobuf 4.25.5 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0m"]}]},{"cell_type":"code","source":["#%%writefile CYH_(model_load)_v1.py\n","\n","import torch\n","import numpy as np\n","from PIL import Image\n","from IPython.display import display\n","from transformers import ViTForImageClassification,AutoFeatureExtractor,ViTImageProcessor  # or the appropriate model class\n","\n","# Load the model from the output directory\n","ASL_VIT_model_v1= ViTForImageClassification.from_pretrained(\"/content/drive/MyDrive/Work_space/Project/Can_You_Hear/API/app/function01/model/ASL_(sign_to_text)_VIT_model_v1\")\n","ASL_VIT_model_v1.eval()\n","\n","ASL_VIT_model_v1_processor = ViTImageProcessor.from_pretrained(\"/content/drive/MyDrive/Work_space/Project/Can_You_Hear/API/app/function01/model/ASL_(sign_to_text)_VIT_model_v1\")\n","\n","id2label_ASL_VIT_model_v1 = {0: 'A', 1: 'B', 2: 'C', 3: 'D', 4: 'DEL', 5: 'E', 6: 'F', 7: 'G', 8: 'H', 9: 'I', 10: 'J', 11: 'K',\n","            12: 'L', 13: 'M', 14: 'N', 15: 'O', 16: 'P', 17: 'Q', 18: 'R', 19: 'S', 20: 'SPACE', 21: 'T', 22: 'U',\n","            23: 'V', 24: 'W', 25: 'X', 26: 'Y', 27: 'Z'}\n","\n","def ASL_VIT_model_v1_predict(image):\n","\n","    # Load an image and feature extractor (preprocessing for vision models)\n","    #image = Image.open(image)\n","\n","    # Display the image\n","    #display(image)\n","\n","    # Preprocess the image\n","    inputs = ASL_VIT_model_v1_processor(images=image, return_tensors=\"pt\")\n","\n","    # Get predictions\n","    with torch.no_grad():\n","        outputs = ASL_VIT_model_v1(**inputs)\n","        logits = outputs.logits\n","        score=np.sum(abs(logits.detach().numpy()))\n","        #print(score*10)\n","        predictions = torch.argmax(logits, dim=-1)\n","        #print(predictions)\n","\n","    #print(f\"Predicted class: {id2label_ASL_VIT_model_v1[predictions.item()]}\")\n","    return id2label_ASL_VIT_model_v1[predictions.item()],score"],"metadata":{"id":"1cQfgKWFKMFM","executionInfo":{"status":"ok","timestamp":1730702302372,"user_tz":-330,"elapsed":485,"user":{"displayName":"Kaushi Gihan ML","userId":"11214181140146971518"}}},"execution_count":48,"outputs":[]},{"cell_type":"code","source":["ASL_VIT_model_v1_predict(\"/content/v1.jpg\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"njHzRKjmLHEM","executionInfo":{"status":"ok","timestamp":1730699242443,"user_tz":-330,"elapsed":2088,"user":{"displayName":"Kaushi Gihan ML","userId":"11214181140146971518"}},"outputId":"0be63aa1-20bd-45eb-846b-643d3cc68be0"},"execution_count":34,"outputs":[{"output_type":"execute_result","data":{"text/plain":["('V', 24.614597)"]},"metadata":{},"execution_count":34}]},{"cell_type":"code","source":["#%%writefile CYH_(model_inference)_v1.py\n","import cv2\n","from google.colab.patches import cv2_imshow\n","import mediapipe as mp\n","import numpy as np\n","import json\n","\n","#from CYH_(model_load)_v1 import ASL_VIT_model_v1_predict\n","\n","def resize_with_padding(image, desired_size):\n","    # Load the image\n","    #image = cv2_imread(image_path)\n","    #cv2_imshow(image)\n","    # Get the current dimensions\n","    h, w = image.shape[:2]\n","\n","    # Calculate the aspect ratio\n","    aspect_ratio = w / h\n","    # Determine new dimensions keeping the aspect ratio\n","    if aspect_ratio > 1:  # width is greater than height\n","        new_w = desired_size[0]\n","        new_h = int(new_w / aspect_ratio)\n","    elif aspect_ratio == 1:\n","        new_w = desired_size[0]\n","        new_h = desired_size[0]\n","    else:  # height is greater than width\n","        new_h = desired_size[1]\n","        new_w = int(new_h * aspect_ratio)\n","\n","    # Resize the image\n","    resized_image = cv2.resize(image, (new_w, new_h))\n","\n","    # Calculate padding to make the image the desired size\n","    delta_w = desired_size[0] - new_w\n","    delta_h = desired_size[1] - new_h\n","    top, bottom = delta_h // 2, delta_h - (delta_h // 2)\n","    left, right = delta_w // 2, delta_w - (delta_w // 2)\n","\n","    # Pad the image\n","    color = [225,225,225]  # Padding color (black)\n","    padded_image = cv2.copyMakeBorder(resized_image, top, bottom, left, right, cv2.BORDER_CONSTANT, value=color)\n","    return padded_image\n","\n","def get_fps_opencv(video_path):\n","    cap = cv2.VideoCapture(video_path)\n","    fps = cap.get(cv2.CAP_PROP_FPS)\n","    cap.release()\n","    return fps\n","\n","\n","def image_prediction(data=None,kill:int=0):\n","    #data= image decode data from client side\n","    #kill=destroy hte ll cv2 windows\n","\n","    # Initialize MediaPipe Hands and drawing utilities\n","    mp_hands = mp.solutions.hands\n","    mp_drawing = mp.solutions.drawing_utils\n","    mp_drawing_styles = mp.solutions.drawing_styles\n","\n","    cutoff=30\n","    desired_size=(300,300,3)\n","\n","\n","    # Choose font and scale\n","    font = cv2.FONT_HERSHEY_SIMPLEX\n","    font_scale = 1\n","    font_thickness = 2\n","\n","\n","    #final sent create variable\n","    temp_charecter=[]\n","    final_charecter=[]\n","    words=[]\n","    charecter=''\n","    sentences=[]\n","\n","\n","    # Initialize the Hands model\n","    hands = mp_hands.Hands(static_image_mode=True, max_num_hands=1, min_detection_confidence=0.3)\n","\n","\n","    #nparr = np.frombuffer(data, np.uint8)\n","    frame = cv2.imread(data)\n","    try:\n","        if frame is not None:\n","            score=''\n","\n","\n","            image_rgb  = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n","\n","            # Read an image\n","            #cv2.imshow(image_rgb)\n","\n","            # Process the image to detect hand landmarks\n","            results = hands.process(image_rgb)\n","\n","            # Check if any hands are detected\n","            if results.multi_hand_landmarks:\n","                alphabetical_character=\" \"\n","                for hand_landmarks in results.multi_hand_landmarks:\n","                    # Get the landmarks\n","                    landmarks = hand_landmarks.landmark\n","                    #print(landmarks)\n","                    h, w, _ = image_rgb.shape\n","                    #print(h,w)\n","\n","                    for i, landmark in enumerate(landmarks):\n","                        x, y, z = int(landmark.x * w), int(landmark.y * h), landmark.z\n","                        #print(f\"Landmark {i}: (x: {x}, y: {y}, z: {z})\")\n","\n","                    # Initialize bounding box coordinates\n","                    min_x, min_y = w, h\n","                    max_x, max_y = 0, 0\n","\n","                    # Iterate through landmarks to find the bounding box\n","                    for landmark in landmarks:\n","                        x, y = int(landmark.x * w), int(landmark.y * h)\n","                        min_x, min_y = min(min_x, x), min(min_y, y)\n","                        max_x, max_y = max(max_x, x), max(max_y, y)\n","\n","\n","\n","                    #Define X Y margine\n","                    if min_x-cutoff < 0:\n","                        minx=0\n","                    else:\n","                        minx=min_x-cutoff\n","\n","                    if min_y-cutoff < 0:\n","                        miny=0\n","                    else:\n","                        miny=min_y-cutoff\n","\n","                    if max_x+cutoff > w:\n","                        maxx=max_x\n","                    else:\n","                        maxx=max_x+cutoff\n","\n","                    if max_y+cutoff > h:\n","                        maxy=max_y\n","                    else:\n","                        maxy=max_y+cutoff\n","\n","                    # Draw the bounding box\n","                    #cv2.rectangle(image_rgb , (minx, miny), (maxx, maxy), (0, 255, 0), 2)\n","                    #cv2_imshow(frame)\n","\n","                    # Crop the image to the bounding box\n","                    cropped_image = image_rgb [miny:maxy,minx:maxx]\n","\n","                    if cropped_image is not None and cropped_image.size != 0:\n","                        try:\n","\n","                            charecter_VIT,score=ASL_VIT_model_v1_predict(cropped_image)\n","\n","                            charecter=charecter_VIT\n","\n","                            #resize image\n","                            resize_image=resize_with_padding(cropped_image, desired_size)\n","                            # Process the image to detect hand landmarks\n","                            results2 = hands.process(resize_image)\n","                            #cv2.imshow(\"image_crop\",resize_image)\n","\n","                            # Check if any hands are detected\n","                            if results2.multi_hand_landmarks:\n","                                #print(\"ok\")\n","                                for hand_landmarks2 in results2.multi_hand_landmarks:\n","                                    # Get the landmarks\n","                                    landmarks2 = hand_landmarks2.landmark\n","                                    #print(landmarks)\n","                                    h2, w2, _ = resize_image.shape\n","                                    #print(h2,w2)\n","\n","                                    #define x y storage location\n","                                    x_lists = []\n","                                    y_lists = []\n","                                    z_lists = []\n","\n","                                    # Initialize bounding box coordinates\n","                                    min_x2, min_y2 = w2, h2\n","                                    max_x2, max_y2 = 0, 0\n","\n","                                    for i, landmark in enumerate(landmarks2):\n","                                        x, y, z = int(landmark.x * w2), int(landmark.y * h2), int(landmark.z*100)\n","                                        x_lists.append(x)\n","                                        y_lists.append(y)\n","                                        z_lists.append(z)\n","                                        #print(f\"Landmark {i}: (x: {x}, y: {y}, z: {z})\")\n","\n","                                    # Iterate through landmarks to find the bounding box\n","                                    for landmark in landmarks:\n","                                        x, y = int(landmark.x * w2), int(landmark.y * h2)\n","                                        min_x2, min_y2 = min(min_x2, x), min(min_y2, y)\n","                                        max_x2, max_y2 = max(max_x2, x), max(max_y2, y)\n","\n","                                    list_pred=[]\n","                                    list_pred.extend(x_lists)\n","                                    list_pred.extend(y_lists)\n","                                    list_pred.extend(z_lists)\n","\n","\n","\n","                                    # Optional: Draw landmarks and connections\n","                                    mp_drawing.draw_landmarks(\n","                                        resize_image,hand_landmarks2, mp_hands.HAND_CONNECTIONS,\n","                                        mp_drawing_styles.get_default_hand_landmarks_style(),\n","                                        mp_drawing_styles.get_default_hand_connections_style())\n","\n","                                    #resize_image_rbg=cv2.cvtColor(resize_image, cv2.COLOR_BGR2RGB)\n","                                    #cv2_imshow(resize_image)\n","                                    #get prediction\n","                                    #charecter_rf=CatBC_model(list_pred)\n","\n","\n","                                    # Display the output image with bounding box\n","                                    #cv2.imshow(\"sub frame_2\",resize_image)\n","                                    #cv2_imshow(resize_image)\n","                                    #cv2.waitKey(1)\n","\n","\n","                        except ValueError as e:\n","                            print(e)\n","                    else:\n","                        print(\"croped image is None\")\n","\n","\n","\n","                # Optional: Draw landmarks and connections\n","                mp_drawing.draw_landmarks(\n","                    frame,hand_landmarks, mp_hands.HAND_CONNECTIONS,\n","                    mp_drawing_styles.get_default_hand_landmarks_style(),\n","                    mp_drawing_styles.get_default_hand_connections_style())\n","\n","\n","            # Add the text\n","            present_charecter=f\" present charecter :{charecter}\"\n","\n","            print(present_charecter)\n","\n","            cv2.putText(frame,present_charecter, (10,90), font, font_scale, (0, 255, 0), font_thickness)\n","\n","\n","            #cv2.imshow(\"Main_frame\",frame)\n","            cv2_imshow(frame)\n","            # Listen to the keyboard for presses.\n","            keyboard_input = cv2.waitKey(1)\n","\n","            # Yield the processed result as a JSON string\n","            #yield f\"data: {json.dumps({'Present charecter': present_charecter,'Final charecters':final_charecters})}\\n\\n\"\n","            status=1\n","            return status,charecter,score\n","            # Add a small delay to simulate real-time processing\n","            #await asyncio.sleep(0.1)\n","\n","        elif kill == 1:\n","            #destroy hte ll cv2 windows\n","            cv2.destroyAllWindows() # Close the OpenCV window when the connection is closed\n","\n","        else:\n","            print(\"image is None\")\n","            Massage=\"image is None\"\n","            status=0\n","            return status,charecter,Massage,\n","\n","    except Exception as e:\n","        print(f\"frame error: {e}\")\n","        Massage=f\"frame error: {e}\"\n","        return status,charecter,Massage\n"],"metadata":{"id":"sZ7m_GafqTSv","executionInfo":{"status":"ok","timestamp":1730702306321,"user_tz":-330,"elapsed":543,"user":{"displayName":"Kaushi Gihan ML","userId":"11214181140146971518"}}},"execution_count":49,"outputs":[]},{"cell_type":"code","source":["status,charecter,score=image_prediction(data=\"/content/hand2_d_bot_seg_\",kill=0)\n","print(\"status\",status)\n","print(f\"present charecter :{charecter}\")\n","print(f\"score :{score}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aRUxlXgHL3UH","executionInfo":{"status":"ok","timestamp":1730702326757,"user_tz":-330,"elapsed":754,"user":{"displayName":"Kaushi Gihan ML","userId":"11214181140146971518"}},"outputId":"deae6c0e-a372-47ab-b025-4e1e89628f50"},"execution_count":51,"outputs":[{"output_type":"stream","name":"stdout","text":["image is None\n","status 0\n","present charecter :\n","score :image is None\n"]}]},{"cell_type":"code","source":["import sys\n","print(cv2.__version__)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tHi3DQh97dTC","executionInfo":{"status":"ok","timestamp":1730703341006,"user_tz":-330,"elapsed":5,"user":{"displayName":"Kaushi Gihan ML","userId":"11214181140146971518"}},"outputId":"0c979a31-23c3-4d6d-c693-a63d70af2b88"},"execution_count":53,"outputs":[{"output_type":"stream","name":"stdout","text":["4.10.0\n"]}]}]}